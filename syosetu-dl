#!/usr/bin/python3

import time,re,requests,sys,os,shutil,argparse
import urllib.parse
from bs4 import BeautifulSoup as bs4
from jinja2 import Environment, FileSystemLoader

THEME="default"

CONFIG_DIR=os.path.dirname(os.path.abspath(__file__))+"/"
STATIC_DIR=CONFIG_DIR+"themes/"+THEME+"/"
BASE_PATH="base.html"
INDEX_PATH="index.html"
SINGLE_PATH="single.html"

def raise_error(e,exit=True):
	print(e,file=sys.stderr)
	if exit:
		sys.exit(1)

def get_data(url):
	headers={"User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0"}
	cookie={'over18': 'yes'}
	data=requests.get(url=url,headers=headers,cookies=cookie)
	if int(data.status_code/100)==5:
		raise_error("Network Error (5xx)")
	elif int(data.status_code/100)==4:
		raise_error("The novel not found")
	return data.content

parser=argparse.ArgumentParser()
parser.add_argument('arg',help="default:url")
parser.add_argument('-a',"--axel",action='store_true',help="turn on axceleration mode")
parser.add_argument('-d',"--dir",default="",help="set download directory")
parser.add_argument('-r',"--renew",action='store_true',help="force to update all files")
parser.add_argument('-e',"--episode",default="",help="set download episode")
parser.add_argument("--without-style",action='store_true',help="generate single page with css (with -e option)")
#parser.add_argument('-o',"--output",default="",help="set download filename (with single novel or -e option)")
args=parser.parse_args()

env=Environment(loader=FileSystemLoader(STATIC_DIR,encoding='utf8'))
base_html=env.get_template(BASE_PATH)
index_html=env.get_template(INDEX_PATH)
single_html=env.get_template(SINGLE_PATH)
ret=urllib.parse.urlparse(args.arg)
base_url="https://"+ret.hostname+"/"
if re.match(r'.*syosetu.com',ret.hostname):
	type="narou"
	ncode=re.match(r'/(n[0-9a-zA-Z]+)',ret.path).group(1)
elif re.match(r'.*kakuyomu.jp',ret.hostname):
	type="kakuyomu"
	ncode=re.match(r'/works/([0-9]+)',ret.path).group(1)
else:
	print("That url is not supported")
	sys.exit()
if args.axel:
	lazy=0
else:
	lazy=1


if type=="narou":
	info_res = get_data(base_url+ncode)
	top_data = bs4(info_res,"html.parser")
	index_raw=top_data.select_one(".index_box")
	if index_raw or not args.episode:
		raws=index_raw.select(".novel_sublist2")
		num_parts=len(raws)
	else:
		num_parts=1
		if not args.dir:
			ndir=""
	title=top_data.select_one("title").text

elif type=="kakuyomu":
	info_res = get_data(base_url+"works/"+ncode)
	top_data = bs4(info_res,"html.parser")
	index_raw=top_data.select_one(".widget-toc-items")
	raws=index_raw.select("li.widget-toc-episode")
	num_parts = len(raws)
	title=top_data.select_one("#workTitle").text

if args.episode:
	if not re.match(r'^\d*$',args.episode):
		raise_error("Incorrect Episode number")
	elif int(args.episode)>num_parts or int(args.episode)<0:
		args.episode=""

novels=[]
if num_parts==1 or args.episode:
	if args.dir:
		ndir=os.path.abspath(args.dir)+"/"
		if not os.path.isdir(ndir):
			os.mkdir(ndir)
	else:
		ndir=""

else:
	if args.dir:
		ndir=os.path.abspath(args.dir)+"/"
		if not os.path.isdir(ndir):
			os.mkdir(ndir)
		else:
			files=os.listdir(ndir)
			for file in files:
				base,ext=os.path.splitext(file)
				if ext=='.html':
					novels.append(base)
	else:
		ndir=os.getcwd()+"/"+ncode+"/"
		if not os.path.isdir(ndir):
			os.mkdir(ndir)
		else:
			files=os.listdir(ndir)
			for file in files:
				if os.path.splitext(file)[1]=='.html':
					novels.append(base)


if num_parts==1 or args.episode:
	style=""
	files=os.listdir(STATIC_DIR+"static/")
	for file in files:
		if os.path.splitext(file)[1]==".css":
			with open(STATIC_DIR+"static/"+file,"r") as f:
				style=style+f.read()

	if type=="narou":
		if args.episode:
			data=get_data(base_url+ncode+"/"+args.episode)
			soup=bs4(data,"html.parser")
			body=soup.select_one("#novel_honbun")
			subtitle=soup.select_one(".novel_subtitle").text
		else:
			body=top_data.select_one("#novel_honbun")
		l=[bs4(str(i),"html.parser") for i in body("p")]
		[i.p.unwrap() for i in l]
		body=[str(i) for i in l]

	elif type=="kakuyomu":
		if args.episode:
			try:
				url=base_url+raws[int(args.episode)].a['href'].replace("/","",1)
			except IndexError:
				raise_error("Incorrect episode number")
		else:
			url=base_url+raws[0].find('a')['href'].replace("/","",1)
		res = get_data(url=url)
		soup = bs4(res,"html.parser")
		subtitle=soup.select_one(".widget-episodeTitle").text
		body=soup.select_one(".widget-episodeBody")
		l=[bs4(str(i),"html.parser") for i in body("p")]
		[i.p.unwrap() for i in l]
		body=[str(i) for i in l]

	if args.episode:
		part=int(args.episode)
		if part==1:
			ppart="#"
			npart=part+1
		elif part==num_parts:
			ppart=part-1
			npart="#"
		else:
			ppart=part-1
			npart=part+1
		if args.without_style:
			contents = base_html.render(title=title,subtitle=subtitle,part=part,total=num_parts,ppart=ppart,npart=npart,contents=body,lines=len(body))
		else:
			contents = base_html.render(title=title,subtitle=subtitle,part=part,total=num_parts,ppart=ppart,npart=npart,contents=body,lines=len(body),style=style)
		with open(ndir+args.episode+".html", "w", encoding="utf-8") as f:
			f.write(contents)
	else:
		contents=single_html.render(title=title,contents=body,style=style,lines=len(body))
		with open(ndir+title+".html", "w", encoding="utf-8") as f:
			f.write(contents)
	print("part 1 downloaded (total: {:d} parts)".format(num_parts))
	sys.exit()


if type=="narou":
	eles=bs4(str(index_raw).replace("\n",""),"html.parser").contents[0].contents
	index=[]
	part=1
	for ele in eles:
		if re.match(r'.+chapter_title',str(ele)):
			index.append({'type': 1,'text': ele.text})
		elif re.match(r'.+novel_sublist2',str(ele)):
			index.append({'type': 2,'text': ele.a.text,'part': part,'time': ele.dt.text.replace("（改）","")})
			part=part+1
	desc="".join([str(i) for i in top_data.select_one("#novel_ex").contents])

elif type=="kakuyomu":
	eles=bs4(str(index_raw).replace("\n",""),"html.parser").contents[0].contents
	index=[]
	part=1
	for ele in eles:
		if re.match(r'.+widget-toc-chapter',str(ele)):
			index.append({'type': 1,'text': ele.text})
		elif re.match(r'.+widget-toc-episode',str(ele)):
			timestamp=re.sub(r':\d\dZ$','',ele.time['datetime'].replace("-","/",2).replace("T"," ",1))
			index.append({'type': 2,'text': ele.span.text, 'part': part, 'time': timestamp})
			part=part+1
	desc=top_data.select_one("#introduction")
	if desc.select_one(".ui-truncateTextButton-expandButton"):
		desc.select_one(".ui-truncateTextButton-expandButton").decompose()
		desc.span.unwrap()
	desc="".join([str(i) for i in desc.contents])
contents=index_html.render(title=title,desc=desc,index=index)
with open(ndir+"/index.html","w") as f:
		f.write(contents)

if not os.path.islink(ndir+"static"):
	if os.path.isdir(ndir+"static"):
		shutil.rmtree(ndir+"static")
	elif os.path.isfile(ndir+"static"):
		os.remove(ndir+"static")
	#os.symlink(STATIC_DIR+"static",ndir+"static")
	os.mkdir(ndir+"static")
	files=os.listdir(STATIC_DIR+"static")
	for file in files:
		with open(STATIC_DIR+"static/"+file,"r") as f1:
			with open(ndir+"static/"+file,"w") as f2:
				f2.write(f1.read())


for part in range(1, num_parts+1):

	if str(part) in novels and not args.renew:
		print("part {:d} skipped (total: {:d} parts)".format(part, num_parts))
		continue

	if type=="narou":
		res = get_data(base_url+ncode+"/{:d}/".format(part))
		soup = bs4(res,"html.parser")
		subtitle=soup.select_one(".novel_subtitle").text
		body=soup.select_one("#novel_honbun")
		l=[bs4(str(i),"html.parser") for i in body("p")]
		[i.p.unwrap() for i in l]
		body=[str(i) for i in l]

	elif type=="kakuyomu":
		res = get_data(base_url+raws[part-1].find('a')['href'].replace("/","",1))
		soup = bs4(res,"html.parser")
		subtitle=soup.select_one(".widget-episodeTitle").text
		body=soup.select_one(".widget-episodeBody")
		l=[bs4(str(i),"html.parser") for i in body("p")]
		[i.p.unwrap() for i in l]
		body=[str(i) for i in l]

	if part==1:
		ppart="#"
		npart=part+1
	elif part==num_parts:
		ppart=part-1
		npart="#"
	else:
		ppart=part-1
		npart=part+1

	contents = base_html.render(title=title,subtitle=subtitle,part=part,total=num_parts,ppart=ppart,npart=npart,contents=body,lines=len(body))

	with open(ndir+str(part)+".html", "w", encoding="utf-8") as f:
			f.write(contents)

	print("part {:d} downloaded (total: {:d} parts)".format(part, num_parts))
	time.sleep(lazy)


