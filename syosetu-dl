#!/usr/bin/python3

import time,re,requests,sys,os,shutil,argparse
import urllib.parse
from bs4 import BeautifulSoup as bs4
from jinja2 import Environment, FileSystemLoader

THEME="default"

CONFIG_DIR=os.path.dirname(os.path.abspath(__file__))+"/"
STATIC_DIR=CONFIG_DIR+"themes/"+THEME+"/"
BASE_PATH="base.html"
INDEX_PATH="index.html"
SINGLE_PATH="single.html"

parser=argparse.ArgumentParser()
parser.add_argument('arg',help="default:url")
parser.add_argument('-a',"--axel",action='store_true',help="turn on axceleration mode")
parser.add_argument('-d',"--dir",default="",help="set download directory")
parser.add_argument('-r',"--renew",action='store_true',help="force to update all files")
args=parser.parse_args()

ret=urllib.parse.urlparse(args.arg)
if re.match(r'.*syosetu.com',ret.hostname):
	type="narou"
	ncode=re.match(r'/(n[0-9a-zA-Z]+)',ret.path).group(1)
elif re.match(r'.*kakuyomu.jp',ret.hostname):
	type="kakuyomu"
	ncode=re.match(r'/works/([0-9]+)',ret.path).group(1)
else:
	print("That url is not supported")
	sys.exit()

if args.axel:
	lazy=0
else:
	lazy=1

if args.dir:
	ndir=os.path.abspath(args.dir)+"/"
else:
	ndir=os.getcwd()+"/"+ncode+"/"

env=Environment(loader=FileSystemLoader(STATIC_DIR,encoding='utf8'))
base_html=env.get_template(BASE_PATH)
index_html=env.get_template(INDEX_PATH)
single_html=env.get_template(SINGLE_PATH)

base_url="https://"+ret.hostname+"/"
headers={"User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0"}
cookie={'over18': 'yes'}

if type=="narou":
	info_url = base_url+ncode
	info_res = requests.get(url=info_url,headers=headers,cookies=cookie).content
	top_data = bs4(info_res,"html.parser")
	index_raw=top_data.select_one(".index_box")
	if index_raw:
		raws=index_raw.select(".novel_sublist2")
		num_parts=len(raws)
	else:
		num_parts=1
		if not args.dir:
			ndir=""
	title=top_data.select_one("title").text

elif type=="kakuyomu":
	info_url = base_url+"works/"+ncode
	info_res = requests.get(url=info_url,headers=headers).content
	top_data = bs4(info_res,"html.parser")
	index_raw=top_data.select_one(".widget-toc-items")
	raws=index_raw.select("li.widget-toc-episode")
	num_parts = len(raws)
	title=top_data.select_one("#workTitle").text

novels=[]
if os.path.isdir(ndir) and num_parts!=1:
	files=os.listdir(ndir)
	for file in files:
		base,ext=os.path.splitext(file)
		if ext=='.txt' or ext=='.html':
			novels.append(base)
elif ndir!="":
	os.mkdir(ndir)

if num_parts==1:
	style=""
	files=os.listdir(STATIC_DIR+"static/")
	for file in files:
		if os.path.splitext(file)[1]==".css":
			with open(STATIC_DIR+"static/"+file,"r") as f:
				style=style+f.read()

	if type=="narou":
		body=top_data.select_one("#novel_honbun")
		body=[i.contents[0] for i in body("p")]

	elif type=="kakuyomu":
		url=base_url+raws[0].find('a')['href'].replace("/","",1)
		res = requests.get(url=url,headers=headers).content
		soup = bs4(res,"html.parser")
		subtitle=soup.select_one(".widget-episodeTitle").text
		body=soup.select_one(".widget-episodeBody")
		body=[i.contents[0] for i in body("p")]
	contents=single_html.render(title=title,contents=body,style=style,lines=len(body))
	with open(ndir+title+".html", "w", encoding="utf-8") as f:
		f.write(contents)
	print("part 1 downloaded (total: {:d} parts)".format(num_parts))
	sys.exit()


if type=="narou":
	eles=bs4(str(index_raw).replace("\n",""),"html.parser").contents[0].contents
	index=[]
	part=1
	for ele in eles:
		if re.match(r'.+chapter_title',str(ele)):
			index.append({'type': 1,'text': ele.text})
		elif re.match(r'.+novel_sublist2',str(ele)):
			index.append({'type': 2,'text': ele.a.text,'part': part,'time': ele.dt.text.replace("（改）","")})
			part=part+1
	desc="".join([str(i) for i in top_data.select_one("#novel_ex").contents])

elif type=="kakuyomu":
	eles=bs4(str(index_raw).replace("\n",""),"html.parser").contents[0].contents
	index=[]
	part=1
	for ele in eles:
		if re.match(r'.+widget-toc-chapter',str(ele)):
			index.append({'type': 1,'text': ele.text})
		elif re.match(r'.+widget-toc-episode',str(ele)):
			timestamp=re.sub(r':\d\dZ$','',ele.time['datetime'].replace("-","/",2).replace("T"," ",1))
			index.append({'type': 2,'text': ele.span.text, 'part': part, 'time': timestamp})
			part=part+1
	desc=top_data.select_one("#introduction")
	if desc.select_one(".ui-truncateTextButton-expandButton"):
		desc.select_one(".ui-truncateTextButton-expandButton").decompose()
		desc.span.unwrap()
	desc="".join([str(i) for i in desc.contents])
contents=index_html.render(title=title,desc=desc,index=index)
with open(ndir+"/index.html","w") as f:
		f.write(contents)

if not os.path.islink(ndir+"static"):
	if os.path.isdir(ndir+"static"):
		shutil.rmtree(ndir+"static")
	elif os.path.isfile(ndir+"static"):
		os.remove(ndir+"static")
	#os.symlink(STATIC_DIR+"static",ndir+"static")
	os.mkdir(ndir+"static")
	files=os.listdir(STATIC_DIR+"static")
	for file in files:
		with open(STATIC_DIR+"static/"+file,"r") as f1:
			with open(ndir+"static/"+file,"w") as f2:
				f2.write(f1.read())


for part in range(1, num_parts+1):

	if str(part) in novels and not args.renew:
		print("part {:d} skipped (total: {:d} parts)".format(part, num_parts))
		continue

	if type=="narou":
		url = base_url+"{}/{:d}/".format(ncode, part)
		res = requests.get(url=url,headers=headers,cookies=cookie).content
		soup = bs4(res,"html.parser")
		subtitle=soup.select_one(".novel_subtitle").text
		body=soup.select_one("#novel_honbun")
		body=[i.contents[0] for i in body("p")]

	elif type=="kakuyomu":
		url = base_url+raws[part-1].find('a')['href'].replace("/","",1)
		res = requests.get(url=url,headers=headers).content
		soup = bs4(res,"html.parser")
		subtitle=soup.select_one(".widget-episodeTitle").text
		body=soup.select_one(".widget-episodeBody")
		body=[i.contents[0] for i in body("p")]

	if part==1:
		ppart="#"
		npart=part+1
	elif part==num_parts:
		ppart=part-1
		npart="#"
	else:
		ppart=part-1
		npart=part+1

	contents = base_html.render(title=title,subtitle=subtitle,part=part,total=num_parts,ppart=ppart,npart=npart,contents=body,lines=len(body))

	with open(ndir+str(part)+".html", "w", encoding="utf-8") as f:
			f.write(contents)

	print("part {:d} downloaded (total: {:d} parts)".format(part, num_parts))
	time.sleep(lazy)


